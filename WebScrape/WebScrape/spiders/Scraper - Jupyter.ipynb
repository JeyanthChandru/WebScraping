{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scrapy\n",
    "import numpy as np\n",
    "import os\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pandas - For creating as a dataframe\n",
    "#matplotlib - For plotting the values\n",
    "#scrapy - Library for running the program and crea ting a spider for crawl and scrape\n",
    "#numpy - Getting as a numpy array for easy operations\n",
    "#os - For checking the existence of the file\n",
    "#CrawlerRunner - Executing a crawler from Python\n",
    "#reactor - Executing scrapy scripts within python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "namesList = np.array([])\n",
    "#for storing the names scraped from the Webpage\n",
    "locationList = np.array([])\n",
    "#for storing the location scraped from the Webpage\n",
    "londonLocation = {}\n",
    "bayAreaLocation = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class namesSpider(scrapy.Spider):\n",
    "    name = \"names\"\n",
    "    \n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'https://www.kennet.com/who-we-are/'\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        namesList = response.css('h3.line.listheader a::text').extract()\n",
    "        locationList = response.css('span.location::text').extract()\n",
    "        for i, loc in enumerate(locationList):\n",
    "            if loc == \"London\":\n",
    "                londonLocation[namesList[i]] = loc\n",
    "            elif loc == \"Silicon Valley\":\n",
    "                bayAreaLocation[namesList[i]] = loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#namesSpider - name of the spider created for gathering names and locations from the Webpage. Also, it will be useful for running the crawler.\n",
    "#start_requests() - Requesting to do the crawling operation listed for the url's.\n",
    "#parse() - Scraping the response to obtain our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = CrawlerRunner()\n",
    "d = runner.crawl(namesSpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLondon = pd.DataFrame(list(londonLocation.items()), columns=['Name','Location'])\n",
    "print(dfLondon)\n",
    "dfBayArea = pd.DataFrame(list(bayAreaLocation.items()), columns=['Name', 'Location'])\n",
    "print(dfBayArea)\n",
    "labels = dfLondon['Location'][0], dfBayArea['Location'][0]\n",
    "sizes = dfLondon.shape[0], dfBayArea.shape[0]\n",
    "colors = ['blue', 'green']\n",
    "patches, texts = plt.pie(sizes, labels = labels, colors = colors)\n",
    "plt.legend(patches, labels, loc = 'best')\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
